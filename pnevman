pip install pycharm-community
pip install tensorflow
python -m venv myenv
myenv\Scripts\activate
import pandas as pd
import os
from matplotlib import pyplot as plt
from PIL import Image
metadata_path = "D:/chest_xray_metadata.csv"
metadata = pd.read_csv(metadata_path)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
data_iterator = train.as_numpy_iterator()
batch = data_iterator.next()
fig, ax = plt.subplots(ncols=4, figsize=(20,20))
for idx, img in enumerate(batch[0][:4]):
    ax[idx].imshow(img.astype(int))
    ax[idx].title.set_text(batch[1][idx])
x_train = []
y_train = []
x_val = []
y_val = []
x_test = []
y_test = []
for feature, label in train:
    x_train.append(feature.numpy())
    y_train.append(label.numpy())

for feature, label in test:
    x_test.append(feature.numpy())
    y_test.append(label.numpy())

for feature, label in validation:
    x_val.append(feature.numpy())
    y_val.append(label.numpy())
    Списки с признаками (x_train, x_val

    x_train = np.concatenate(x_train, axis=0)
x_val = np.concatenate(x_val, axis=0)
x_test = np.concatenate(x_test, axis=0)
y_train = np.concatenate(y_train, axis=0)
y_val = np.concatenate(y_val, axis=0)
y_test = np.concatenate(y_test, axis=0)
print("Shape of 'x_train':", x_train.shape)
print("Shape of 'y_train':", y_train.shape)
print("Shape of 'x_val':", x_val.shape)
print("Shape of 'y_val':", y_val.shape)
print("Shape of 'x_test':", x_test.shape)
print("Shape of 'y_test':", y_test.shape)
Shape of 'x_train': (5216, 256, 256, 3)
Shape of 'y_train': (5216, 2)
Shape of 'x_val': (16, 256, 256, 3)
Shape of 'y_val': (16, 2)
Shape of 'x_test': (624, 256, 256, 3)
Shape of 'y_test': (624, 2)

x_train=x_train/256
x_val=x_val/256
x_test=x_test/256
#  создание нейросети
def preprocess_images(image_list):
    """
    Convert images to grayscale and resize.
    """
    processed_images = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in image_list]
    resized_images = [cv2.resize(img, (128, 128)) for img in processed_images]
    return np.array(resized_images)

  def split_dataset(x_data, y_data, train_size=0.8):
         """
         Split data into training and testing sets.
         """
         total_samples = len(x_data)
         train_samples = int(total_samples * train_size)
     return x_data[:train_samples], y_data[:train_samples], x_data[train_samples:], y_data[train_samples:]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

# Добавление слоев к модели
model.add(Dense(units=64,activation='relu',
input_shape=(input_dim,)))
model.add(Dense(units=32, activation='relu'))
model.add(Dense(units=output_dim, activation='softmax'))
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D

def create_pneumonia_detection_model(input_shape):
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

# Example usage, assuming input shape of images is (128, 128, 1) for grayscale images
model = create_pneumonia_detection_model(input_shape=(128, 128, 1))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))
После создания модели ее компи
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_normalized = scaler.fit_transform(X_train)
X_val_normalized = scaler.transform(X_val)
 model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
 model.fit(X_train_normalized, y_train, epochs=10, batch_size=32, validation_data=(X_val_normalized, y_val))

 import matplotlib.pyplot as plt

def plot_learning_curves(history):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


plot_learning_curves(history)
from sklearn.metrics import confusion_matrix
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title('Матрица ошибок')
    plt.xlabel('Предсказанные значения')
    plt.ylabel('Истинные значения')
    plt.show()

plot_confusion_matrix(y_true, y_pred, classes=['Класс 0', 'Класс 1'])

 Очистка ресурсов TensorFlow
K.clear_session()

# Удаление созданной модели
del model
# Освобождение занятых ресурсов GPU (если используется GPU)
tf.config.experimental.list_physical_devices('GPU')

# Сохранение обученной модели
model.save('trained_model.h5')
